"use strict";(globalThis.webpackChunkdocumentation_docs_markdown=globalThis.webpackChunkdocumentation_docs_markdown||[]).push([[7546],{8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>i});var t=r(6540);const a={},s=t.createContext(a);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(s.Provider,{value:n},e.children)}},9052:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"projects/proof-of-concepts/poc-ingestion-java","title":"POC Ingestion Java","description":"This file documents the Proof of Concept (POC) data ingestion project built with Java and Spring Boot.","source":"@site/docs/projects/proof-of-concepts/poc-ingestion-java.md","sourceDirName":"projects/proof-of-concepts","slug":"/projects/proof-of-concepts/poc-ingestion-java","permalink":"/documentation-docs-markdown/docs/projects/proof-of-concepts/poc-ingestion-java","draft":false,"unlisted":false,"editUrl":"https://github.com/EndToEndLabCR/docs/projects/proof-of-concepts/poc-ingestion-java.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Portfolio Project Tasks","permalink":"/documentation-docs-markdown/docs/projects/portfolio/tasks"},"next":{"title":"Template API Python","permalink":"/documentation-docs-markdown/docs/projects/templates/template-api-python"}}');var a=r(4848),s=r(8453);const o={},i="POC Ingestion Java",c={},d=[{value:"Project Overview",id:"project-overview",level:2},{value:"Tech Stack",id:"tech-stack",level:2},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Project Structure",id:"project-structure",level:2},{value:"Key Components",id:"key-components",level:2},{value:"Kafka Configuration",id:"kafka-configuration",level:3},{value:"Data Ingestion Consumer",id:"data-ingestion-consumer",level:3},{value:"Data Processing Service",id:"data-processing-service",level:3},{value:"Batch Processing Configuration",id:"batch-processing-configuration",level:3},{value:"Monitoring and Metrics",id:"monitoring-and-metrics",level:2},{value:"Custom Metrics",id:"custom-metrics",level:3},{value:"Testing",id:"testing",level:2},{value:"Integration Tests with TestContainers",id:"integration-tests-with-testcontainers",level:3},{value:"Performance Characteristics",id:"performance-characteristics",level:2},{value:"Throughput Metrics",id:"throughput-metrics",level:3},{value:"Scalability Features",id:"scalability-features",level:3},{value:"Deployment",id:"deployment",level:2},{value:"Docker Compose",id:"docker-compose",level:3},{value:"Learning Outcomes",id:"learning-outcomes",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"poc-ingestion-java",children:"POC Ingestion Java"})}),"\n",(0,a.jsx)(n.p,{children:"This file documents the Proof of Concept (POC) data ingestion project built with Java and Spring Boot."}),"\n",(0,a.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,a.jsx)(n.p,{children:"The POC Ingestion Java project demonstrates large-scale data ingestion capabilities using modern Java technologies. It's designed to handle high-volume data processing with fault tolerance, monitoring, and scalability in mind."}),"\n",(0,a.jsx)(n.h2,{id:"tech-stack",children:"Tech Stack"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Framework"}),": Spring Boot 3.x"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Java Version"}),": Java 17+"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Message Broker"}),": Apache Kafka"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Database"}),": PostgreSQL + MongoDB"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Caching"}),": Redis"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Monitoring"}),": Micrometer + Prometheus"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Processing"}),": Spring Batch"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Testing"}),": TestContainers, JUnit 5"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Data Sources \u2192 Kafka \u2192 Ingestion Service \u2192 Processing \u2192 Storage\n                \u2193\n           Dead Letter Queue\n                \u2193\n           Error Handling Service\n"})}),"\n",(0,a.jsx)(n.h2,{id:"project-structure",children:"Project Structure"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"poc-ingestion-java/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main/\n\u2502   \u2502   \u251c\u2500\u2500 java/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 com/endtoendlabcr/ingestion/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 config/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 KafkaConfig.java\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 DatabaseConfig.java\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 BatchConfig.java\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 consumer/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 DataIngestionConsumer.java\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 ErrorConsumer.java\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 processor/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 DataProcessor.java\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 ValidationProcessor.java\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 TransformationProcessor.java\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 model/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 RawData.java\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 ProcessedData.java\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 ErrorData.java\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 repository/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 DataRepository.java\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 ErrorRepository.java\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 service/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 IngestionService.java\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 ProcessingService.java\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 MonitoringService.java\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 IngestionApplication.java\n\u2502   \u2502   \u2514\u2500\u2500 resources/\n\u2502   \u2502       \u251c\u2500\u2500 application.yml\n\u2502   \u2502       \u2514\u2500\u2500 db/migration/\n\u2502   \u2514\u2500\u2500 test/\n\u251c\u2500\u2500 docker/\n\u2502   \u251c\u2500\u2500 kafka/\n\u2502   \u251c\u2500\u2500 postgres/\n\u2502   \u2514\u2500\u2500 monitoring/\n\u251c\u2500\u2500 pom.xml\n\u2514\u2500\u2500 docker-compose.yml\n"})}),"\n",(0,a.jsx)(n.h2,{id:"key-components",children:"Key Components"}),"\n",(0,a.jsx)(n.h3,{id:"kafka-configuration",children:"Kafka Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-java",children:'@Configuration\n@EnableKafka\n@Slf4j\npublic class KafkaConfig {\n\n    @Value("${spring.kafka.bootstrap-servers}")\n    private String bootstrapServers;\n\n    @Bean\n    public ConsumerFactory<String, String> consumerFactory() {\n        Map<String, Object> props = new HashMap<>();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, "ingestion-group");\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 100);\n        \n        return new DefaultKafkaConsumerFactory<>(props);\n    }\n\n    @Bean\n    public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {\n        ConcurrentKafkaListenerContainerFactory<String, String> factory = \n            new ConcurrentKafkaListenerContainerFactory<>();\n        factory.setConsumerFactory(consumerFactory());\n        factory.setConcurrency(3); // Number of consumer threads\n        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE);\n        factory.setErrorHandler(new SeekToCurrentErrorHandler(\n            new DeadLetterPublishingRecoverer(kafkaTemplate()), \n            new FixedBackOff(1000L, 3)\n        ));\n        \n        return factory;\n    }\n\n    @Bean\n    public KafkaTemplate<String, String> kafkaTemplate() {\n        return new KafkaTemplate<>(producerFactory());\n    }\n\n    @Bean\n    public ProducerFactory<String, String> producerFactory() {\n        Map<String, Object> props = new HashMap<>();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        props.put(ProducerConfig.ACKS_CONFIG, "all");\n        props.put(ProducerConfig.RETRIES_CONFIG, 3);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);\n        props.put(ProducerConfig.LINGER_MS_CONFIG, 5);\n        \n        return new DefaultKafkaProducerFactory<>(props);\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"data-ingestion-consumer",children:"Data Ingestion Consumer"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-java",children:'@Component\n@Slf4j\n@RequiredArgsConstructor\npublic class DataIngestionConsumer {\n\n    private final IngestionService ingestionService;\n    private final MeterRegistry meterRegistry;\n    private final Counter processedMessagesCounter;\n    private final Counter errorCounter;\n\n    @KafkaListener(topics = "${kafka.topic.raw-data}", groupId = "ingestion-group")\n    public void consume(\n            @Payload String message,\n            @Header Map<String, Object> headers,\n            Acknowledgment acknowledgment,\n            ConsumerRecord<String, String> consumerRecord) {\n        \n        Timer.Sample sample = Timer.start(meterRegistry);\n        \n        try {\n            log.info("Received message: key={}, partition={}, offset={}", \n                    consumerRecord.key(), \n                    consumerRecord.partition(), \n                    consumerRecord.offset());\n\n            // Process the message\n            RawData rawData = parseMessage(message, headers);\n            ingestionService.processData(rawData);\n            \n            // Acknowledge successful processing\n            acknowledgment.acknowledge();\n            processedMessagesCounter.increment();\n            \n            log.debug("Successfully processed message with key: {}", consumerRecord.key());\n            \n        } catch (ValidationException e) {\n            log.warn("Validation failed for message: {}", e.getMessage());\n            handleValidationError(message, headers, e);\n            acknowledgment.acknowledge(); // Acknowledge to avoid reprocessing\n            errorCounter.increment("validation");\n            \n        } catch (ProcessingException e) {\n            log.error("Processing failed for message: {}", e.getMessage(), e);\n            handleProcessingError(message, headers, e);\n            acknowledgment.acknowledge();\n            errorCounter.increment("processing");\n            \n        } catch (Exception e) {\n            log.error("Unexpected error processing message: {}", e.getMessage(), e);\n            errorCounter.increment("unexpected");\n            throw e; // Let Kafka retry mechanism handle it\n            \n        } finally {\n            sample.stop(Timer.builder("kafka.message.processing.time")\n                .description("Time taken to process Kafka message")\n                .register(meterRegistry));\n        }\n    }\n\n    private RawData parseMessage(String message, Map<String, Object> headers) {\n        try {\n            ObjectMapper mapper = new ObjectMapper();\n            RawData rawData = mapper.readValue(message, RawData.class);\n            \n            // Add metadata from headers\n            rawData.setTimestamp(Instant.now());\n            rawData.setSource(headers.get("source").toString());\n            \n            return rawData;\n        } catch (JsonProcessingException e) {\n            throw new ValidationException("Invalid JSON format: " + e.getMessage());\n        }\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"data-processing-service",children:"Data Processing Service"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-java",children:'@Service\n@Transactional\n@Slf4j\n@RequiredArgsConstructor\npublic class IngestionService {\n\n    private final DataRepository dataRepository;\n    private final ValidationProcessor validationProcessor;\n    private final TransformationProcessor transformationProcessor;\n    private final RedisTemplate<String, Object> redisTemplate;\n\n    public void processData(RawData rawData) {\n        // Step 1: Validate incoming data\n        ValidationResult validationResult = validationProcessor.validate(rawData);\n        if (!validationResult.isValid()) {\n            throw new ValidationException("Data validation failed: " + \n                String.join(", ", validationResult.getErrors()));\n        }\n\n        // Step 2: Check for duplicates using Redis cache\n        String deduplicationKey = generateDeduplicationKey(rawData);\n        if (Boolean.TRUE.equals(redisTemplate.hasKey(deduplicationKey))) {\n            log.warn("Duplicate data detected, skipping: {}", deduplicationKey);\n            return;\n        }\n\n        // Step 3: Transform data\n        ProcessedData processedData = transformationProcessor.transform(rawData);\n\n        // Step 4: Enrich with additional data\n        enrichData(processedData);\n\n        // Step 5: Store in database\n        dataRepository.save(processedData);\n\n        // Step 6: Cache for deduplication\n        redisTemplate.opsForValue().set(\n            deduplicationKey, \n            true, \n            Duration.ofHours(24)\n        );\n\n        log.info("Successfully processed and stored data with ID: {}", processedData.getId());\n    }\n\n    private String generateDeduplicationKey(RawData rawData) {\n        return "dedup:" + DigestUtils.md5Hex(\n            rawData.getSource() + ":" + \n            rawData.getExternalId() + ":" + \n            rawData.getTimestamp().toString()\n        );\n    }\n\n    private void enrichData(ProcessedData processedData) {\n        // Add business logic for data enrichment\n        if (processedData.getLocation() != null) {\n            // Geocoding or location-based enrichment\n            LocationData locationData = locationService.enrichLocation(processedData.getLocation());\n            processedData.setEnrichedLocation(locationData);\n        }\n\n        // Add calculated fields\n        processedData.setProcessingTimestamp(Instant.now());\n        processedData.setDataQualityScore(calculateQualityScore(processedData));\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"batch-processing-configuration",children:"Batch Processing Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-java",children:'@Configuration\n@EnableBatchProcessing\n@RequiredArgsConstructor\npublic class BatchConfig {\n\n    private final JobBuilderFactory jobBuilderFactory;\n    private final StepBuilderFactory stepBuilderFactory;\n    private final DataSource dataSource;\n\n    @Bean\n    public Job dataReprocessingJob() {\n        return jobBuilderFactory.get("dataReprocessingJob")\n            .incrementer(new RunIdIncrementer())\n            .flow(reprocessingStep())\n            .end()\n            .build();\n    }\n\n    @Bean\n    public Step reprocessingStep() {\n        return stepBuilderFactory.get("reprocessingStep")\n            .<ErrorData, ProcessedData>chunk(100)\n            .reader(errorDataReader())\n            .processor(reprocessingProcessor())\n            .writer(processedDataWriter())\n            .faultTolerant()\n            .skipLimit(10)\n            .skip(ProcessingException.class)\n            .build();\n    }\n\n    @Bean\n    @StepScope\n    public JdbcCursorItemReader<ErrorData> errorDataReader() {\n        return new JdbcCursorItemReaderBuilder<ErrorData>()\n            .name("errorDataReader")\n            .dataSource(dataSource)\n            .sql("SELECT * FROM error_data WHERE status = \'PENDING\' ORDER BY created_at")\n            .rowMapper(new BeanPropertyRowMapper<>(ErrorData.class))\n            .build();\n    }\n\n    @Bean\n    public ItemProcessor<ErrorData, ProcessedData> reprocessingProcessor() {\n        return errorData -> {\n            try {\n                // Attempt to reprocess the error data\n                RawData rawData = reconstructRawData(errorData);\n                return transformationProcessor.transform(rawData);\n            } catch (Exception e) {\n                log.error("Failed to reprocess error data: {}", errorData.getId(), e);\n                return null; // Skip this item\n            }\n        };\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"monitoring-and-metrics",children:"Monitoring and Metrics"}),"\n",(0,a.jsx)(n.h3,{id:"custom-metrics",children:"Custom Metrics"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-java",children:'@Component\n@RequiredArgsConstructor\npublic class IngestionMetrics {\n\n    private final MeterRegistry meterRegistry;\n\n    @PostConstruct\n    public void initMetrics() {\n        // Register custom gauges\n        Gauge.builder("ingestion.queue.size")\n            .description("Current size of the ingestion queue")\n            .register(meterRegistry, this, IngestionMetrics::getQueueSize);\n\n        Gauge.builder("ingestion.processing.rate")\n            .description("Current processing rate per second")\n            .register(meterRegistry, this, IngestionMetrics::getProcessingRate);\n    }\n\n    public double getQueueSize() {\n        // Implementation to get queue size\n        return queueService.getCurrentSize();\n    }\n\n    public double getProcessingRate() {\n        // Implementation to calculate processing rate\n        return metricsService.getProcessingRate();\n    }\n\n    @EventListener\n    public void handleDataProcessed(DataProcessedEvent event) {\n        Counter.builder("data.processed.total")\n            .tag("source", event.getSource())\n            .tag("type", event.getDataType())\n            .description("Total number of processed data records")\n            .register(meterRegistry)\n            .increment();\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"testing",children:"Testing"}),"\n",(0,a.jsx)(n.h3,{id:"integration-tests-with-testcontainers",children:"Integration Tests with TestContainers"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-java",children:'@SpringBootTest\n@Testcontainers\nclass IngestionIntegrationTest {\n\n    @Container\n    static PostgreSQLContainer<?> postgres = new PostgreSQLContainer<>("postgres:14")\n            .withDatabaseName("testdb")\n            .withUsername("test")\n            .withPassword("test");\n\n    @Container\n    static KafkaContainer kafka = new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:latest"));\n\n    @Container\n    static GenericContainer<?> redis = new GenericContainer<>("redis:7-alpine")\n            .withExposedPorts(6379);\n\n    @DynamicPropertySource\n    static void configureProperties(DynamicPropertyRegistry registry) {\n        registry.add("spring.datasource.url", postgres::getJdbcUrl);\n        registry.add("spring.datasource.username", postgres::getUsername);\n        registry.add("spring.datasource.password", postgres::getPassword);\n        registry.add("spring.kafka.bootstrap-servers", kafka::getBootstrapServers);\n        registry.add("spring.redis.host", redis::getHost);\n        registry.add("spring.redis.port", redis::getFirstMappedPort);\n    }\n\n    @Autowired\n    private KafkaTemplate<String, String> kafkaTemplate;\n\n    @Autowired\n    private DataRepository dataRepository;\n\n    @Test\n    void shouldProcessKafkaMessage() throws Exception {\n        // Given\n        String testMessage = """\n            {\n                "externalId": "test-123",\n                "data": "test data",\n                "source": "test-source"\n            }\n            """;\n\n        // When\n        kafkaTemplate.send("raw-data", "test-key", testMessage).get();\n\n        // Then\n        await().atMost(10, TimeUnit.SECONDS)\n            .untilAsserted(() -> {\n                List<ProcessedData> processedData = dataRepository.findAll();\n                assertThat(processedData).hasSize(1);\n                assertThat(processedData.get(0).getSource()).isEqualTo("test-source");\n            });\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"performance-characteristics",children:"Performance Characteristics"}),"\n",(0,a.jsx)(n.h3,{id:"throughput-metrics",children:"Throughput Metrics"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Target"}),": 10,000 messages/second"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Latency"}),": < 100ms p95"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Error Rate"}),": < 0.1%"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Availability"}),": 99.9%"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"scalability-features",children:"Scalability Features"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Horizontal scaling with Kafka partitions"}),"\n",(0,a.jsx)(n.li,{children:"Database connection pooling"}),"\n",(0,a.jsx)(n.li,{children:"Redis caching for deduplication"}),"\n",(0,a.jsx)(n.li,{children:"Batch processing for reprocessing"}),"\n",(0,a.jsx)(n.li,{children:"Circuit breaker for external services"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"deployment",children:"Deployment"}),"\n",(0,a.jsx)(n.h3,{id:"docker-compose",children:"Docker Compose"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'version: \'3.8\'\n\nservices:\n  ingestion-service:\n    build: .\n    ports:\n      - "8080:8080"\n    environment:\n      - SPRING_PROFILES_ACTIVE=docker\n      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092\n      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/ingestion\n      - SPRING_REDIS_HOST=redis\n    depends_on:\n      - kafka\n      - postgres\n      - redis\n\n  kafka:\n    image: confluentinc/cp-kafka:latest\n    environment:\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n    depends_on:\n      - zookeeper\n\n  zookeeper:\n    image: confluentinc/cp-zookeeper:latest\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n\n  postgres:\n    image: postgres:14\n    environment:\n      POSTGRES_DB: ingestion\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: password\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - "6379:6379"\n\n  prometheus:\n    image: prom/prometheus\n    ports:\n      - "9090:9090"\n    volumes:\n      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml\n'})}),"\n",(0,a.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(n.p,{children:"This POC demonstrates:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Large-scale data ingestion patterns"}),"\n",(0,a.jsx)(n.li,{children:"Kafka integration and error handling"}),"\n",(0,a.jsx)(n.li,{children:"Spring Boot and Spring Batch usage"}),"\n",(0,a.jsx)(n.li,{children:"Database optimization for high throughput"}),"\n",(0,a.jsx)(n.li,{children:"Monitoring and observability"}),"\n",(0,a.jsx)(n.li,{children:"Testing strategies for distributed systems"}),"\n",(0,a.jsx)(n.li,{children:"Performance tuning and scalability considerations"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}}}]);